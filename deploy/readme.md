# Продакшен и деплой

## Деплой на PaaS

PaaS — сервисы, которые полностью скрывают инфраструктуру, но позволяют быстро задеплоить любое приложение. Их преимущества в скорости, надежности и легкости масштабирования.

При этом на приложение накладывается ряд ограничений, а также такие сервисы достаточно дорогие.

[Railway](https://railway.app/) — один из таких сервисов.

[Railway and CI/CD](https://blog.railway.app/p/nx-railway-with-gh-actions)

## 12 факторов

Методология создания веб-приложений (SaaS), которые:

* Используют декларативный формат для описания процесса установки и настройки;
* Максимально переносимы между средами;
* Подходят для развёртывания на облачных платформах;
* Сводят к минимуму расохждаения между средой разработки и выполнения;
* Могут масштабироваться без существенных изменений в инструментах, архитектуре и практие разработки.

Факторы:

1. [Кодовая база](https://habr.com/ru/post/258739/#codebase)
    Одна кодовая база, отслеживаемая в системе контроля версий, – множество развертываний
2. [Зависимости](https://habr.com/ru/post/258739/#dependencies)
    Явно объявляйте и изолируйте зависимости
3. [Конфигурация](https://habr.com/ru/post/258739/#config)
    Сохраняйте конфигурацию в среде выполнения
4. [Сторонние службы (Backing Services)](https://habr.com/ru/post/258739/#backing-services)
    Считайте сторонние службы (backing services) подключаемыми ресурсами
5. [Сборка, релиз, выполнение](https://habr.com/ru/post/258739/#build-release-run)
    Строго разделяйте стадии сборки и выполнения
6. [Процессы](https://habr.com/ru/post/258739/#processes)
    Запускайте приложение как один или несколько процессов не сохраняющих внутреннее состояние (stateless)
7. [Привязка портов (port binding)](https://habr.com/ru/post/258739/#port-binding)
    Экспортируйте сервисы через привязку портов
8. [Параллелизм](https://habr.com/ru/post/258739/#concurrency)
    Масштабируйте приложение с помощью процессов
9. [Одноразовость (Disposability)](https://habr.com/ru/post/258739/#disposability)
    Максимизируйте надежность с помощью быстрого запуска и корректного завершения работы
10. [Паритет разработки/работы приложения](https://habr.com/ru/post/258739/#dev-prod-parity)
     Держите окружения разработки, промежуточного развёртывания (staging) и рабочего развёртывания (production) максимально похожими
11. [Журналирование (Logs)](https://habr.com/ru/post/258739/#logs)
     Рассматривайте журнал как поток событий
12. [Задачи администрирования](https://habr.com/ru/post/258739/#admin-processes)
     Выполняйте задачи администрирования/управления с помощью разовых процессов

[Приложение двенадцати факторов — The Twelve-Factor App](https://habr.com/ru/post/258739/)

[The Twelve-Factor App](https://12factor.net/ru/)

## Сборка проекта

В общем случае процесс деплоя выглядит так:

1. Клонирования репозиторияд;
2. Сборка проекта (например, устаановка зависимостей);
3. Доставка на сервер(а);
4. Остановка старой версии и запуск новой.

С точки зраения 12 факторов важно разделять процесс [сборки и релиза](https://12factor.net/ru/build-release-run).

Сборка обычно выполняется отдельно от релиза. Чаще всего ей занимется CI, он выполняет все проверки и в итоге формирует артефакт: пакет под ОС (например deb), архив, Docker-образ (обычно используется именно он).

## Деплой

В случае деплоя при помощи docker-контейнеров процесс выглядит так:

* Скачивание нужного образа на сервер;
* Остановка старого контейнера;
* Запуск нового контейнера из скачанного образа.

Деплой выполняется с помощью Kubernetes, Ansible и т.п.

## Прокси-сервер

Прокси-сервер ставится между клиентами и внутренней инфраструктурой. Он  принимает запросы от клиентов и переадресует их на внутренний веб-сервер или веб-сервера, если их много. Именно поэтому он называется реверс  (обратный), снаружи внутрь.

Прокси-сервер нужен т.к.:

* Для раздачи статики проекта (css, js, картинки и т.п.), помимо этого нужно их сжимать и кэшировать;
* Многие встроенные веб-сервера плохо работают с "медленными клиентами", эти клиенты долго отправляют и принимают данные, при этом они занимают ресурсы сервера и приложений;
* Встроенные сервера ограничены в своих возможностях (обработка ошибок, педдержка HTTPS и т.д.).

[Что такое обратный прокси](https://ru.hexlet.io/blog/posts/obratnyy-proksi)

## Горизонтальное масштабирование

Горизонтальное масштабирование позволяет решить две задачи:

* Выдержать большую нагрузку;
* Повысить отказоустойчивость системы.

Она используется как для нагруженных так и не нагруженных сайтов, которым важно оставаться работоспособными в случае отказа сервера.

Для горизонтального масштабирования нужно как минимум два сервера, на которые выполняется деплой приложения. Серверов может быть и больше, главное чтобы они были равноправным и взаимозаменяемыми.

В такой схеме остановка одного сервера не приводит к остановке всей системы, нагрузкы перераспределяется на оставшиеся.

Для распределения запросов между серврами используется балансировка. Балансировать нагрузку можно разными способаси.

### DNS балансировка

DNS позволяет добавить любое количество ip-адресов доступных домену. При запросе клиентом домена, браузер получает список ip-адресов, обычно испольщуется только первый из них, на этом факте основана DNS балансировка: DNS сконфигурирован так чтобы возвращать список в случайном порядке, таким образом клиентам переходят по разным ip-адресам.

Такая баланисировка имеет недостатки, существенно ограничивающие надержность и эффективность этого решения: DNS не проверяет доступность серверов, а приложения на роботоспособность, и DNS не уситывает это при возврате списка адресов. Некоторые DNS могут проверять сервера на доступность и убирать из списка недоступные, но тут возниакет проблема с кэшированием DNS, из-за которого какие-то время всё равно будут отдаваться нерабочие адреса.

### Балансировщик нагрузки

Более надежное решение, чем DNS балансировка. Этот механим принимает все входящие запросы и распределяет их между доступными серверами. Балансировщиком нагрузки, как привило, выступает отдельный сервер с Nginx или Caddy, либо облачных сервис.

Он отслеживает доступность серверов, наличие ошибок, а также баалнсирует запросы по нагрузке на серверы и географическому расположению.

Основной недостаток такой схемы — это то, что сам балансировщик может являться узким местом и единой точкой отказа, при выходе его из строя становится недоступна вся система. в в

 Пример балансировки нагрузки на Caddy и Nginx:

```ini
# Caddy
handle {
    # Добавляем столько серверов, сколько нужно
    reverse_proxy app1:3000 app2:3000 app3:3000
}

# Nginx
http {
    server {
        location / {
        # Запрос перенаправляется на upstream backend
            proxy_pass http://backend;
        }
    }
    upstream backend {
    # Добавляем столько серверов, сколько нужно
        server app1;
        server app2;
        server app3;
    }
}
```

### Состояние на сервере

Ключевое ограничение горизонтального масштабирования — отсутвия состояние на сервере. Под состоянием понимаются любые данные: БД, файлы, сессии и т.п.

В случае использования горизонтального масштабирования харнить их на сервере приложения нельзя, иначе они будут доступны на одном сервере, но не доступны на других.

Чтобы решить эту проблему:

* БД должна храниться на своём выделеном сервере;
* Сессии жалательно хранить в куках;
* Файлы хранить на сервисах типа [Amazon S3](https://aws.amazon.com/ru/s3/).

### Логирование

Ещё одна сложность — это легирование, логи появляются на каждом сервере отдельно и работать с ними просто так не удобно, чтобы это исправить существуют два решения:

* Облачное. Используются специальных сервисы для сбора и агрегации логов. Это могут быть как встроенные в инфраструктуру решения (например [Amazon CloudWatch](https://aws.amazon.com/ru/solutions/implementations/centralized-logging/) для Amazon), либо внешние сервисы, например [DataDog](https://www.datadoghq.com/product/log-management/);
* Самостоятельное. Например, связка [ElasticSearch](https://www.elastic.co/) (хранилище и поиск), [Logstash](https://www.elastic.co/logstash/) (сборщик логов) и [Kibana](https://www.elastic.co/kibana/) (инструмент визуализации), иногда её называют [ELK](https://www.elastic.co/elastic-stack/) стек.

## Организация работы с БД

БД — ключевая часть практически любой инфраструктуру.

Обычно компании там где возможно, стараются свести своё участие в управлении БД к минимуму, где возможно используются облачные решения. В таком случае БД предоставляется как сервис, который умеет автоматически делать бэкпы, восстаналвивать их, обеспечивать отказоустойчивость за счёт автоматического переключения на запасную копию БД и многое другое.

Если БД управляют самостоятельно, то она всё равно должна находиться на отдельном сервере. Это важно потому что:

* Чтобы БД не конкурировала за ресурсы сервера;
* У БД свой тип нагрузки, под который нужны сервра со специфическими характеристиками;
* К БД принимаются более высокие стандарты безопасности и надёжности;
* Без этого невозможно горизонтальное масштабирование.

### Миграция

Миграция — процесс изменения БД, её схемы и данных, которые возникают во время разработки.

Во время деплоя миграции накатываются, обычно с помощью команд встраенных в фреймворк приложения.

Бывает что миграцию нужно откатить, для этого должна быть предустрена своя команда, стоит учитывать, что откат не будет происходить сам по себе и есть некоторые действия, которые могут привести к поторере данных (удаление таблиц, колонок), с такими операциями следует действовать осторожно.

### Деплой

При деплое миграции обычно применяют во время запуска новой версии приложения в конце деплоя, это выглядит так:

1. Приложение устанавливается на все сервера;
2. Останавливается старая версия приложения;
3. Накатываются миграции;
4. Запускается новая версия приложения.

Миграции могут ломать работу старого кода, поэтому они накатываются уже после остановки старой версии.

Могут быть миграции, которые накатываются очень долго, тогда их можно выполнить до деплоя, но важно чтобы эти изменения не ломали обратную совместимость. В таком случае порядок деполя такой:

1. Приложение устанавливается на все сервера;
2. Накатываются миграции;
3. Останавливается старая версия приложения;
4. Запускается новая версия приложения.

### Zero Downtime Deploy

Правильная работа с БД, не ломающая обратную совместимость, позволяет промзводить Zero Downtime Deploy, когда сервису не нужно уходить в downtime для обновления. Обычно это делают крупные сервисы, котрым очень важна доступность.

Деплой у них может происходить примерно по такой схема:

1. Скачиваем новую версию приложения на сервера;
2. Выполняем миграции с любого из серверов;
3. Обновляем первый сервер:
   1. Выводим сервер из под балансировщика, чтобы он не отправлял на него запросы;
   2. Останавливаем текущую и запускаем новую версию приложения;
   3. Добавляем сервер в балансировщик.
4. Повторяем тоже самое со вторым сервером.

## Мониторинг

Мониторинг включает с себя три основных элемента:

* Сбор метрик, например, анализ оставшегося места на диске или проверка кодов ответов веб-сервера;
* Визуализация метрик, возможность посмотреть их на графике, поискать корреляции между разными показателямии;
* оповещения (Алертинг) об ошибках или опасных ситуациях.

### Сбор метрик

Сбор данных для мониторинга происходит с помощью специальных приложений (агентов), на для мониторинга на облачных сервисах они могут быть встроенны с саму инфраструктуру. Для внешних систем мониторинга их нужно будет поставить самостояительно.

Все агенты рабтают примерно одинаково, они получают данные от ОС или запущенных на ней программ и отправляют в центральное хранилище. Получение данных происходит с помощью их вытягивания или приёма входящих данных.

Обычно у систем монторинга есть готовые интеграции агентов для разных приложений.

### Вывод графиков

 Данные полученные от агентов попадают в ситему мониторинга и на их основе строятся графики, благодаря готовым игтеграциям система уже знает как работать с данными.

### Алертинг

Для уведолменя о проблемах настраивается алертинг. Каждый алерт следит за какой-то метрикой и срабатывает при откланении её значения от допустимого, при этом важно учитывать переодичечность этих отклонений, промежутки времени, которые наблюдаются отключение и прочее, чтобы избежать ложных срабатываний.

### Self-Hosted решения

Если нельзя использовать готовый сервис мониторинга, то можно собрать собственный используются готовые открые проекты:

* [Prometheus](https://prometheus.io/) - для сбора метрик и алертинга;
* [Grafana](https://grafana.com/) - для вывода графиков.

[Что такое трекинг ошибок](https://guides.hexlet.io/ru/error-tracking/?roistat_visit=7387064)

